{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "    \n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 25), match=\"I'm searching for a spark\"> 0 25\n",
      "<re.Match object; span=(25, 36), match=' in PySpark'> 25 36\n"
     ]
    }
   ],
   "source": [
    "m = re.finditer(r'.*?(spark).*?', \"I'm searching for a spark in PySpark\", re.I)\n",
    "for match in m:\n",
    "    print(match, match.start(), match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark setup \n",
    "It can be setup as described below or through bashrc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.4.5-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.5-bin-hadoop2.7\"\n",
    "print(os.environ.get('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1) Download it if they do not exist locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_dir = \"./data/\"\n",
    "if not os.path.exists(path_to_data_dir):\n",
    "    os.mkdir(path_to_data_dir)\n",
    "    print(\"Created data directory\")\n",
    "\n",
    "if not os.path.exists(\"./data/NASA_access_log_Jul95.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz \n",
    "    !mv ./NASA_access_log_Jul95.gz ./data/\n",
    "\n",
    "if not os.path.exists(\"./data/NASA_access_log_Aug95.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz \n",
    "    !mv ./NASA_access_log_Aug95.gz ./data/\n",
    "\n",
    "if not os.path.exists(\"./data/clarknet_access_log_Aug28.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/clarknet_access_log_Aug28.gz \n",
    "    !mv ./clarknet_access_log_Aug28.gz ./data/\n",
    "\n",
    "if not os.path.exists(\"./data/clarknet_access_log_Sep4.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/clarknet_access_log_Sep4.gz\n",
    "    !mv ./clarknet_access_log_Sep4.gz ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse log files\n",
    "\n",
    "Four gz files were downloaded and they have same format with same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/clarknet_access_log_Sep4.gz', './data/NASA_access_log_Jul95.gz', './data/clarknet_access_log_Aug28.gz', './data/NASA_access_log_Aug95.gz']\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "raw_data_files = glob.glob('./data/*.gz')\n",
    "print(raw_data_files)\n",
    "base_df = spark.read.text(raw_data_files)\n",
    "print(base_df.printSchema())\n",
    "print(type(base_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Converting data frame to Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|204.249.225.59 - - [28/Aug/1995:00:00:34 -0400] \"GET /pub/rmharris/catalogs/dawsocat/intro.html HTTP/1.0\" 200 3542|\n",
      "|access9.accsyst.com - - [28/Aug/1995:00:00:35 -0400] \"GET /pub/robert/past99.gif HTTP/1.0\" 200 4993               |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='204.249.225.59 - - [28/Aug/1995:00:00:34 -0400] \"GET /pub/rmharris/catalogs/dawsocat/intro.html HTTP/1.0\" 200 3542'),\n",
       " Row(value='access9.accsyst.com - - [28/Aug/1995:00:00:35 -0400] \"GET /pub/robert/past99.gif HTTP/1.0\" 200 4993')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.show(2, truncate=False)\n",
    "\n",
    "base_df_rdd = base_df.rdd\n",
    "print(type(base_df_rdd))\n",
    "base_df_rdd.take(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install findspark\n",
    "# os.environ.get('SPARK_HOME')\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "\n",
    "There are multiple columns that are listed below. They have their unique patterns.  \n",
    "\n",
    "1. Host names\n",
    "2. Access timestamps\n",
    "3. HTTP Request method, endpoint, and protocol\n",
    "4. HTTP Status codes\n",
    "5. HTTP Response content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_host = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s' # host name - ['204.249.225.59', 'access9.accsyst.com']\n",
    "pa_time = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]' # date and time - 01/Jul/1995:00:00:01 -0400\n",
    "pa_protocol = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"' # ('GET', '/history/apollo/', 'HTTP/1.0')\n",
    "pa_status = r'\\s(\\d{3})\\s' # status code\n",
    "pa_content_size = r'\\s(\\d+)$' # Content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|               Host|           Timestamp|Method|            Endpoint|Protocol|Status|Content_size|\n",
      "+-------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|     204.249.225.59|28/Aug/1995:00:00...|   GET|/pub/rmharris/cat...|HTTP/1.0|   200|        3542|\n",
      "|access9.accsyst.com|28/Aug/1995:00:00...|   GET|/pub/robert/past9...|HTTP/1.0|   200|        4993|\n",
      "+-------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "(<bound method DataFrame.count of DataFrame[Host: string, Timestamp: string, Method: string, Endpoint: string, Protocol: string, Status: int, Content_size: int]>, 7)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "df_parse_log = base_df.select(regexp_extract('value', pa_host, 1).alias('Host'),\n",
    "                         regexp_extract('value', pa_time, 1).alias('Timestamp'),\n",
    "                         regexp_extract('value', pa_protocol, 1).alias('Method'),\n",
    "                         regexp_extract('value', pa_protocol, 2).alias('Endpoint'),\n",
    "                         regexp_extract('value', pa_protocol, 3).alias('Protocol'),\n",
    "                         regexp_extract('value', pa_status, 1).cast('integer').alias('Status'),\n",
    "                         regexp_extract('value', pa_content_size, 1).cast('integer').alias('Content_size'))\n",
    "df_parse_log.show(2, truncate=True)\n",
    "print((df_parse_log.count, len(df_parse_log.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_day' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-04064e8c1f80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_day\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_day' is not defined"
     ]
    }
   ],
   "source": [
    "df_day.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = df_parse_log.filter(df_parse_log['Host'].isNull()| \n",
    "                                    df_parse_log['Timestamp'].isNull() | \n",
    "                                    df_parse_log['Method'].isNull() |\n",
    "                                    df_parse_log['Endpoint'].isNull() |\n",
    "                                    df_parse_log['Protocol'].isNull() |\n",
    "                                    df_parse_log['Status'].isNull() |\n",
    "                                    df_parse_log['Content_size'].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is distribution of missing values per column?\n",
    "\n",
    "-> Build up a list of column expressions, one per column.\n",
    "\n",
    "-> Run the aggregation. The *exprs converts the list of expressions into\n",
    "\n",
    "-> variable function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "def count_null(col_name):\n",
    "    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "exprs = [count_null(col_name) for col_name in df_parse_log.columns]\n",
    "df_parse_log.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_status_df = base_df.filter(~base_df['value'].rlike(r'\\s(\\d{3})\\s'))\n",
    "null_status_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_status_df = null_status_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                                      regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                                      regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                                      regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "bad_status_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parse_log = df_parse_log[df_parse_log['Status'].isNotNull()]\n",
    "df_parse_log.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [count_null(col_name) for col_name in df_parse_log.columns]\n",
    "df_parse_log.agg(*exprs).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling nulls in HTTP content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_content_size_df = base_df.filter(~base_df['value'].rlike(r'\\s\\d+$'))\n",
    "null_content_size_df.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_content_size_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parse_log = df_parse_log.na.fill({'content_size': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [count_null(col_name) for col_name in logs_df.columns]\n",
    "logs_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Temporal Fields (Timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caution: There is a built in function for casting. In this case, it can be easily cast using timestamp however it keeps giving an error. Therefore, I used following technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "month_str_ints = {'Jan': '1', 'Feb': '2', 'Mar':'3', 'Apr':'4' , 'May': '5', 'Jun':'6', \n",
    "                  'Jul': '7', 'Aug': '8', 'Sep':'9', 'Oct':'10', 'Nov':'11', 'Dec':'12' }\n",
    "\n",
    "def get_month(text):\n",
    "    if not text[3:6] in month_str_ints:\n",
    "        return \"13\"\n",
    "    else:\n",
    "        return \"%s\" %str(month_str_ints[text[3:6]])\n",
    "      \n",
    "def get_year(text):\n",
    "    return \"%s\" %str(text[7:11])\n",
    "\n",
    "def get_day(text):\n",
    "    return \"%s\" %str(text[0:2])\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "udf_parse_time = udf(get_year)\n",
    "df_cleaned = df_parse_log.select('*', udf_parse_time(df_parse_log['Timestamp'])\n",
    "                                .alias('Year'))\n",
    "udf_parse_time = udf(get_month)\n",
    "df_cleaned = df_cleaned.select('*', udf_parse_time(df_parse_log['Timestamp'])\n",
    "                                .alias('Month'))\n",
    "udf_parse_time = udf(get_day)\n",
    "df_cleaned = df_cleaned.select('*', udf_parse_time(df_parse_log['Timestamp'])\n",
    "                                .alias('Day')).drop('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------+------+------------+----+-----+---+\n",
      "|                Host|Method|            Endpoint|Protocol|Status|Content_size|Year|Month|Day|\n",
      "+--------------------+------+--------------------+--------+------+------------+----+-----+---+\n",
      "|      204.249.225.59|   GET|/pub/rmharris/cat...|HTTP/1.0|   200|        3542|1995|    8| 28|\n",
      "| access9.accsyst.com|   GET|/pub/robert/past9...|HTTP/1.0|   200|        4993|1995|    8| 28|\n",
      "| access9.accsyst.com|   GET|/pub/robert/curr9...|HTTP/1.0|   200|        5836|1995|    8| 28|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|       18338|1995|    8| 28|\n",
      "|    cssu24.cs.ust.hk|   GET|/pub/job/vk/view1...|HTTP/1.0|   200|        5944|1995|    8| 28|\n",
      "|     er6.rutgers.edu|   GET|/pub/rjgula/netwo...|HTTP/1.0|   200|        2017|1995|    8| 28|\n",
      "|cyclom1-1-6.inter...|   GET|/pub/k2/jeep/jxj.htm|HTTP/1.0|   200|        3254|1995|    8| 28|\n",
      "|d24-1.cpe.Brisban...|   GET|/pub/eurocent/hom...|HTTP/1.0|   200|        2534|1995|    8| 28|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|         813|1995|    8| 28|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|       12871|1995|    8| 28|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|         693|1995|    8| 28|\n",
      "|    cssu24.cs.ust.hk|   GET|/pub/job/vk/view1...|HTTP/1.0|   200|        6578|1995|    8| 28|\n",
      "|d24-1.cpe.Brisban...|   GET|/pub/eurocent/tin...|HTTP/1.0|   200|        1152|1995|    8| 28|\n",
      "|  ppp19.glas.apc.org|   GET|          /atomicbk/|HTTP/1.0|   304|        null|1995|    8| 28|\n",
      "|  ppp19.glas.apc.org|   GET|/atomicbk/contest...|HTTP/1.0|   304|        null|1995|    8| 28|\n",
      "|  ppp19.glas.apc.org|   GET|/atomicbk/artgal.gif|HTTP/1.0|   304|        null|1995|    8| 28|\n",
      "|  ppp19.glas.apc.org|   GET| /atomicbk/email.gif|HTTP/1.0|   304|        null|1995|    8| 28|\n",
      "|         ari.ari.net|   GET|                   /|HTTP/1.0|   200|        1834|1995|    8| 28|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|         744|1995|    8| 28|\n",
      "|  ppp19.glas.apc.org|   GET| /atomicbk/promo.gif|HTTP/1.0|   304|        null|1995|    8| 28|\n",
      "+--------------------+------+--------------------+--------+------+------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---+\n",
      "|                Host|Month|Day|\n",
      "+--------------------+-----+---+\n",
      "|      204.249.225.59|    8| 28|\n",
      "| access9.accsyst.com|    8| 28|\n",
      "| access9.accsyst.com|    8| 28|\n",
      "|       world.std.com|    8| 28|\n",
      "|    cssu24.cs.ust.hk|    8| 28|\n",
      "|     er6.rutgers.edu|    8| 28|\n",
      "|cyclom1-1-6.inter...|    8| 28|\n",
      "|d24-1.cpe.Brisban...|    8| 28|\n",
      "|       world.std.com|    8| 28|\n",
      "|       world.std.com|    8| 28|\n",
      "|       world.std.com|    8| 28|\n",
      "|    cssu24.cs.ust.hk|    8| 28|\n",
      "|d24-1.cpe.Brisban...|    8| 28|\n",
      "|  ppp19.glas.apc.org|    8| 28|\n",
      "|  ppp19.glas.apc.org|    8| 28|\n",
      "|  ppp19.glas.apc.org|    8| 28|\n",
      "|  ppp19.glas.apc.org|    8| 28|\n",
      "|         ari.ari.net|    8| 28|\n",
      "|       world.std.com|    8| 28|\n",
      "|  ppp19.glas.apc.org|    8| 28|\n",
      "+--------------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|Month|  count|\n",
      "+-----+-------+\n",
      "|    7|1891714|\n",
      "|    8|2621568|\n",
      "|    9|2276962|\n",
      "|   13|    101|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explortory Data Analysis (EDA)\n",
    "\n",
    "## HTTP Status\n",
    "First, unique HTTP status is determined and then find their distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_freq = (logs_df\n",
    "                     .groupBy('status')\n",
    "                     .count()\n",
    "                     .sort('status')\n",
    "                     .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_status_freq = (df_status_freq\n",
    "                         .toPandas()\n",
    "                         .sort_values(by=['count'], ascending=False))\n",
    "\n",
    "df_pd_status_freq['percent'] = df_pd_status_freq['count']/sum(df_pd_status_freq['count'])\n",
    "df_pd_status_freq['log(count)'] = np.log(df_pd_status_freq['count'])\n",
    "df_pd_status_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_status_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent host\n",
    "\n",
    "Identify the top ten host and also compute their proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_freq = (logs_df\n",
    "                     .groupBy('host')\n",
    "                     .count()\n",
    "                     .cache())\n",
    "df_pd_host_freq = (df_host_freq\n",
    "                        .toPandas()\n",
    "                        .sort_values(by=['count'], ascending=False))\n",
    "df_pd_host_freq[\"Percentage\"] = df_pd_host_freq[\"count\"] / sum(df_pd_host_freq[\"count\"])\n",
    "df_pd_host_freq[\"Log(count)\"] = np.log(df_pd_host_freq[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_host_freq.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean    = np.mean(df_pd_host_freq['count'])\n",
    "std     = np.std(df_pd_host_freq['count'])\n",
    "df_pd_host_freq['zscore'] = (df_pd_host_freq['count'] - mean)/std\n",
    "df_pd_host_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What method is most frequently used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method = (logs_df\n",
    "                     .groupBy('method')\n",
    "                     .count()\n",
    "                     .cache())\n",
    "df_pd_method = (df_method\n",
    "                        .toPandas()\n",
    "                        .sort_values(by=['count'], ascending=False))\n",
    "df_pd_method[\"Percentage\"] = 100.00 * (df_pd_method[\"count\"] / sum(df_pd_method[\"count\"]))\n",
    "df_pd_method[\"Log(count)\"] = np.log(df_pd_method[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_method.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular method is GET followed by HEAD. However, 99% of times GET method is used. Superisingly, POST method is almost never used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocol = (logs_df\n",
    "                     .groupBy('protocol')\n",
    "                     .count()\n",
    "                     .cache())\n",
    "df_protocol = (df_protocol\n",
    "                        .toPandas()\n",
    "                        .sort_values(by=['count'], ascending=False))\n",
    "df_protocol[\"Percentage\"] = 100.00 * (df_protocol[\"count\"] / sum(df_protocol[\"count\"]))\n",
    "df_protocol[\"Log(count)\"] = np.log(df_protocol[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+\n",
      "|               Host|day|\n",
      "+-------------------+---+\n",
      "|     204.249.225.59| 28|\n",
      "|access9.accsyst.com| 28|\n",
      "+-------------------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "host_day_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of host per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6790345"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth\n",
    "1z = df_cleaned.select(df_cleaned.Host,df_cleaned.Month, df_cleaned.Day)\n",
    "df_host_month_day.select('*').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "478545"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth\n",
    "df_host_month_day = df_cleaned.select(df_cleaned.Host,df_cleaned.Month, df_cleaned.Day)\n",
    "df_host_month_day.select('*').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_group_by = (host_day_df.groupBy('Month', 'Day').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+\n",
      "|Month|Day| count|\n",
      "+-----+---+------+\n",
      "|    7| 07| 87233|\n",
      "|    8| 21| 55540|\n",
      "|    7| 12| 92536|\n",
      "|    8| 17| 58988|\n",
      "|    7| 11| 80407|\n",
      "|    7| 21| 64629|\n",
      "|    7| 13|134203|\n",
      "|    8| 06| 32420|\n",
      "|    8| 28|306823|\n",
      "|    9| 05|229939|\n",
      "|    8| 30|354082|\n",
      "|    8| 10| 61248|\n",
      "|    7| 01| 64714|\n",
      "|    7| 27| 61680|\n",
      "|    8| 07| 57362|\n",
      "|    8| 20| 32963|\n",
      "|    9| 07|274660|\n",
      "|    7| 20| 66593|\n",
      "|    8| 25| 57321|\n",
      "|    9| 10|192265|\n",
      "+-----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_host_group_by.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_month_day_unique = df_host_month_day.select('*').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_month_day_unique_gb = (df_host_month_day_unique.groupBy('Month', 'Day').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "|Month|Day|count|\n",
      "+-----+---+-----+\n",
      "|    7| 07| 6471|\n",
      "|    8| 21| 4128|\n",
      "|    7| 12| 5339|\n",
      "|    8| 17| 4382|\n",
      "|    7| 11| 4923|\n",
      "|    7| 21| 4337|\n",
      "|    7| 13| 6946|\n",
      "|    8| 06| 2537|\n",
      "|    8| 28|20979|\n",
      "|    9| 05|16206|\n",
      "|    8| 30|23158|\n",
      "|    8| 10| 4520|\n",
      "|    7| 01| 5191|\n",
      "|    7| 27| 4367|\n",
      "|    8| 20| 2560|\n",
      "|    8| 07| 4104|\n",
      "|    9| 07|19167|\n",
      "|    7| 20| 4725|\n",
      "|    8| 25| 4404|\n",
      "|    9| 10|14414|\n",
      "+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_host_month_day_unique_gb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df_gb_host_month_day = (host_day_df\n",
    "                        .groupBy('Host','Month', 'Day')\n",
    "                        .count()\n",
    "                        .sort(col(\"count\").desc())\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---+-----+\n",
      "|                Host|Month|Day|count|\n",
      "+--------------------+-----+---+-----+\n",
      "|www-e3.proxy.aol.com|    9| 06| 2632|\n",
      "|www-e2.proxy.aol.com|    9| 06| 2045|\n",
      "|intgate.raleigh.i...|    9| 08| 1895|\n",
      "|      acme.clark.net|    9| 02| 1780|\n",
      "|           clark.net|    8| 30| 1512|\n",
      "|mpngate1.ny.us.ib...|    8| 30| 1462|\n",
      "|    webgate1.mot.com|    8| 30| 1444|\n",
      "|www-e3.proxy.aol.com|    9| 07| 1424|\n",
      "|    webgate1.mot.com|    8| 31| 1407|\n",
      "|piweba5y.prodigy.com|    9| 09| 1397|\n",
      "|   bill.ksc.nasa.gov|    7| 11| 1394|\n",
      "|   indy.gradient.com|    7| 12| 1356|\n",
      "| siltb10.orl.mmc.com|    7| 21| 1354|\n",
      "|mpngate1.ny.us.ib...|    9| 06| 1344|\n",
      "|   bill.ksc.nasa.gov|    7| 12| 1317|\n",
      "|  reboot.dt.navy.mil|    9| 07| 1304|\n",
      "|piweba5y.prodigy.com|    9| 04| 1301|\n",
      "|piweba3y.prodigy.com|    7| 16| 1280|\n",
      "|piweba4y.prodigy.com|    7| 16| 1269|\n",
      "|piweba5y.prodigy.com|    8| 31| 1258|\n",
      "+--------------------+-----+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gb_host_month_day.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pyspark_venv': venv)",
   "language": "python",
   "name": "python37564bitpysparkvenvvenv12caecce4d7944089893a0cc0cfe69ee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
