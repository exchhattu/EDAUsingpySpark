{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "    \n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 25), match=\"I'm searching for a spark\"> 0 25\n",
      "<re.Match object; span=(25, 36), match=' in PySpark'> 25 36\n"
     ]
    }
   ],
   "source": [
    "m = re.finditer(r'.*?(spark).*?', \"I'm searching for a spark in PySpark\", re.I)\n",
    "for match in m:\n",
    "    print(match, match.start(), match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark setup \n",
    "It can be setup as described below or through bashrc file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark-2.4.5-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.5-bin-hadoop2.7\"\n",
    "print(os.environ.get('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "1) Download it if they do not exist locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_dir = \"./data/\"\n",
    "if not os.path.exists(path_to_data_dir):\n",
    "    os.mkdir(path_to_data_dir)\n",
    "    print(\"Created data directory\")\n",
    "\n",
    "if not os.path.exists(\"./data/NASA_access_log_Jul95.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz \n",
    "    !mv ./NASA_access_log_Jul95.gz ./data/\n",
    "\n",
    "if not os.path.exists(\"./data/NASA_access_log_Aug95.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz \n",
    "    !mv ./NASA_access_log_Aug95.gz ./data/\n",
    "\n",
    "if not os.path.exists(\"./data/clarknet_access_log_Aug28.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/clarknet_access_log_Aug28.gz \n",
    "    !mv ./clarknet_access_log_Aug28.gz ./data/\n",
    "\n",
    "if not os.path.exists(\"./data/clarknet_access_log_Sep4.gz\"):\n",
    "    !wget ftp://ita.ee.lbl.gov/traces/clarknet_access_log_Sep4.gz\n",
    "    !mv ./clarknet_access_log_Sep4.gz ./data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse log files\n",
    "\n",
    "Four gz files were downloaded and they have same format with same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/clarknet_access_log_Sep4.gz', './data/NASA_access_log_Jul95.gz', './data/clarknet_access_log_Aug28.gz', './data/NASA_access_log_Aug95.gz']\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "None\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "raw_data_files = glob.glob('./data/*.gz')\n",
    "print(raw_data_files)\n",
    "base_df = spark.read.text(raw_data_files)\n",
    "print(base_df.printSchema())\n",
    "print(type(base_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "Converting data frame to Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|value                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "|204.249.225.59 - - [28/Aug/1995:00:00:34 -0400] \"GET /pub/rmharris/catalogs/dawsocat/intro.html HTTP/1.0\" 200 3542|\n",
      "|access9.accsyst.com - - [28/Aug/1995:00:00:35 -0400] \"GET /pub/robert/past99.gif HTTP/1.0\" 200 4993               |\n",
      "+------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(value='204.249.225.59 - - [28/Aug/1995:00:00:34 -0400] \"GET /pub/rmharris/catalogs/dawsocat/intro.html HTTP/1.0\" 200 3542'),\n",
       " Row(value='access9.accsyst.com - - [28/Aug/1995:00:00:35 -0400] \"GET /pub/robert/past99.gif HTTP/1.0\" 200 4993')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.show(2, truncate=False)\n",
    "\n",
    "base_df_rdd = base_df.rdd\n",
    "print(type(base_df_rdd))\n",
    "base_df_rdd.take(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install findspark\n",
    "# os.environ.get('SPARK_HOME')\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "\n",
    "There are multiple columns that are listed below. They have their unique patterns.  \n",
    "\n",
    "1. Host names\n",
    "2. Access timestamps\n",
    "3. HTTP Request method, endpoint, and protocol\n",
    "4. HTTP Status codes\n",
    "5. HTTP Response content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_host = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s' # host name - ['204.249.225.59', 'access9.accsyst.com']\n",
    "pa_time = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]' # date and time - 01/Jul/1995:00:00:01 -0400\n",
    "pa_protocol = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"' # ('GET', '/history/apollo/', 'HTTP/1.0')\n",
    "pa_status = r'\\s(\\d{3})\\s' # status code\n",
    "pa_content_size = r'\\s(\\d+)$' # Content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|               Host|           Timestamp|Method|            Endpoint|Protocol|Status|Content_size|\n",
      "+-------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|     204.249.225.59|28/Aug/1995:00:00...|   GET|/pub/rmharris/cat...|HTTP/1.0|   200|        3542|\n",
      "|access9.accsyst.com|28/Aug/1995:00:00...|   GET|/pub/robert/past9...|HTTP/1.0|   200|        4993|\n",
      "+-------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "(<bound method DataFrame.count of DataFrame[Host: string, Timestamp: string, Method: string, Endpoint: string, Protocol: string, Status: int, Content_size: int]>, 7)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "df_parse_log = base_df.select(regexp_extract('value', pa_host, 1).alias('Host'),\n",
    "                         regexp_extract('value', pa_time, 1).alias('Timestamp'),\n",
    "                         regexp_extract('value', pa_protocol, 1).alias('Method'),\n",
    "                         regexp_extract('value', pa_protocol, 2).alias('Endpoint'),\n",
    "                         regexp_extract('value', pa_protocol, 3).alias('Protocol'),\n",
    "                         regexp_extract('value', pa_status, 1).cast('integer').alias('Status'),\n",
    "                         regexp_extract('value', pa_content_size, 1).cast('integer').alias('Content_size'))\n",
    "df_parse_log.show(2, truncate=True)\n",
    "print((df_parse_log.count, len(df_parse_log.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = df_parse_log.filter(df_parse_log['Host'].isNull()| \n",
    "                                    df_parse_log['Timestamp'].isNull() | \n",
    "                                    df_parse_log['Method'].isNull() |\n",
    "                                    df_parse_log['Endpoint'].isNull() |\n",
    "                                    df_parse_log['Protocol'].isNull() |\n",
    "                                    df_parse_log['Status'].isNull() |\n",
    "                                    df_parse_log['Content_size'].isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|              Host|           Timestamp|Method|            Endpoint|Protocol|Status|Content_size|\n",
      "+------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|ppp19.glas.apc.org|28/Aug/1995:00:00...|   GET|          /atomicbk/|HTTP/1.0|   304|        null|\n",
      "|ppp19.glas.apc.org|28/Aug/1995:00:00...|   GET|/atomicbk/contest...|HTTP/1.0|   304|        null|\n",
      "+------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_missing_data.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is distribution of missing values per column?\n",
    "\n",
    "-> Build up a list of column expressions, one per column.\n",
    "\n",
    "-> Run the aggregation. The *exprs converts the list of expressions into\n",
    "\n",
    "-> variable function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|Host|Timestamp|Method|Endpoint|Protocol|Status|Content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|   101|      308120|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "def count_null(col_name):\n",
    "    return spark_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "exprs = [count_null(col_name) for col_name in df_parse_log.columns]\n",
    "df_parse_log.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|rodan29.pixi.com ...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_status_df = base_df.filter(~base_df['value'].rlike(r'\\s(\\d{3})\\s'))\n",
    "null_status_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'host_pattern' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-21f1e23b0fc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bad_status_df = null_status_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                       \u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timestamp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                       \u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_uri_protocol_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'method'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                       \u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_uri_protocol_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'endpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_uri_protocol_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'protocol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'host_pattern' is not defined"
     ]
    }
   ],
   "source": [
    "bad_status_df = null_status_df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                                      regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                                      regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                                      regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                                      regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "bad_status_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parse_log = df_parse_log[df_parse_log['Status'].isNotNull()]\n",
    "df_parse_log.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [count_null(col_name) for col_name in df_parse_log.columns]\n",
    "df_parse_log.agg(*exprs).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling nulls in HTTP content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_content_size_df = base_df.filter(~base_df['value'].rlike(r'\\s\\d+$'))\n",
    "null_content_size_df.show(1, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_content_size_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_parse_log = df_parse_log.na.fill({'content_size': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprs = [count_null(col_name) for col_name in logs_df.columns]\n",
    "logs_df.agg(*exprs).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Temporal Fields (Timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "month_str_ints = {'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4 , 'May': 5, 'Jun':6, \n",
    "                  'Jul': 7, 'Aug': 8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12 }\n",
    "\n",
    "def parse_clf_time(text):\n",
    "    \"\"\"\n",
    "      Keep date and time hours, minutes and seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_time = \"{0:04d}/{1:02d}/{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n",
    "                      int(text[7:11]),\n",
    "                      month_str_ints[text[3:6]],\n",
    "                      int(text[0:2]),\n",
    "                      int(text[12:14]),\n",
    "                      int(text[15:17]),\n",
    "                      int(text[18:20]))\n",
    "    return cleaned_time\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "udf_parse_time = udf(parse_clf_time)\n",
    "a = df_parse_log.select('*', unix_timestamp(udf_parse_time(df_parse_log['Timestamp']), \"yyyy/MM/dd HH:mm:ss\")\n",
    "                                .cast(TimestampType())\n",
    "                                .alias('Time')).drop('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+--------+------+------------+-------------------+\n",
      "|                Host|Method|            Endpoint|Protocol|Status|Content_size|               Time|\n",
      "+--------------------+------+--------------------+--------+------+------------+-------------------+\n",
      "|      204.249.225.59|   GET|/pub/rmharris/cat...|HTTP/1.0|   200|        3542|1995-08-28 00:00:34|\n",
      "| access9.accsyst.com|   GET|/pub/robert/past9...|HTTP/1.0|   200|        4993|1995-08-28 00:00:35|\n",
      "| access9.accsyst.com|   GET|/pub/robert/curr9...|HTTP/1.0|   200|        5836|1995-08-28 00:00:35|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|       18338|1995-08-28 00:00:36|\n",
      "|    cssu24.cs.ust.hk|   GET|/pub/job/vk/view1...|HTTP/1.0|   200|        5944|1995-08-28 00:00:36|\n",
      "|     er6.rutgers.edu|   GET|/pub/rjgula/netwo...|HTTP/1.0|   200|        2017|1995-08-28 00:00:37|\n",
      "|cyclom1-1-6.inter...|   GET|/pub/k2/jeep/jxj.htm|HTTP/1.0|   200|        3254|1995-08-28 00:00:37|\n",
      "|d24-1.cpe.Brisban...|   GET|/pub/eurocent/hom...|HTTP/1.0|   200|        2534|1995-08-28 00:00:38|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|         813|1995-08-28 00:00:38|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|       12871|1995-08-28 00:00:38|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|         693|1995-08-28 00:00:38|\n",
      "|    cssu24.cs.ust.hk|   GET|/pub/job/vk/view1...|HTTP/1.0|   200|        6578|1995-08-28 00:00:39|\n",
      "|d24-1.cpe.Brisban...|   GET|/pub/eurocent/tin...|HTTP/1.0|   200|        1152|1995-08-28 00:00:40|\n",
      "|  ppp19.glas.apc.org|   GET|          /atomicbk/|HTTP/1.0|   304|        null|1995-08-28 00:00:43|\n",
      "|  ppp19.glas.apc.org|   GET|/atomicbk/contest...|HTTP/1.0|   304|        null|1995-08-28 00:00:43|\n",
      "|  ppp19.glas.apc.org|   GET|/atomicbk/artgal.gif|HTTP/1.0|   304|        null|1995-08-28 00:00:44|\n",
      "|  ppp19.glas.apc.org|   GET| /atomicbk/email.gif|HTTP/1.0|   304|        null|1995-08-28 00:00:44|\n",
      "|         ari.ari.net|   GET|                   /|HTTP/1.0|   200|        1834|1995-08-28 00:00:45|\n",
      "|       world.std.com|   GET|/pub/atomicbk/cat...|HTTP/1.0|   200|         744|1995-08-28 00:00:45|\n",
      "|  ppp19.glas.apc.org|   GET| /atomicbk/promo.gif|HTTP/1.0|   304|        null|1995-08-28 00:00:45|\n",
      "+--------------------+------+--------------------+--------+------+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explortory Data Analysis (EDA)\n",
    "\n",
    "## HTTP Status\n",
    "First, unique HTTP status is determined and then find their distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_status_freq = (logs_df\n",
    "                     .groupBy('status')\n",
    "                     .count()\n",
    "                     .sort('status')\n",
    "                     .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_status_freq = (df_status_freq\n",
    "                         .toPandas()\n",
    "                         .sort_values(by=['count'], ascending=False))\n",
    "\n",
    "df_pd_status_freq['percent'] = df_pd_status_freq['count']/sum(df_pd_status_freq['count'])\n",
    "df_pd_status_freq['log(count)'] = np.log(df_pd_status_freq['count'])\n",
    "df_pd_status_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_status_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent host\n",
    "\n",
    "Identify the top ten host and also compute their proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_host_freq = (logs_df\n",
    "                     .groupBy('host')\n",
    "                     .count()\n",
    "                     .cache())\n",
    "df_pd_host_freq = (df_host_freq\n",
    "                        .toPandas()\n",
    "                        .sort_values(by=['count'], ascending=False))\n",
    "df_pd_host_freq[\"Percentage\"] = df_pd_host_freq[\"count\"] / sum(df_pd_host_freq[\"count\"])\n",
    "df_pd_host_freq[\"Log(count)\"] = np.log(df_pd_host_freq[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_host_freq.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean    = np.mean(df_pd_host_freq['count'])\n",
    "std     = np.std(df_pd_host_freq['count'])\n",
    "df_pd_host_freq['zscore'] = (df_pd_host_freq['count'] - mean)/std\n",
    "df_pd_host_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What method is most frequently used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_method = (logs_df\n",
    "                     .groupBy('method')\n",
    "                     .count()\n",
    "                     .cache())\n",
    "df_pd_method = (df_method\n",
    "                        .toPandas()\n",
    "                        .sort_values(by=['count'], ascending=False))\n",
    "df_pd_method[\"Percentage\"] = 100.00 * (df_pd_method[\"count\"] / sum(df_pd_method[\"count\"]))\n",
    "df_pd_method[\"Log(count)\"] = np.log(df_pd_method[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_method.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most popular method is GET followed by HEAD. However, 99% of times GET method is used. Superisingly, POST method is almost never used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocol = (logs_df\n",
    "                     .groupBy('protocol')\n",
    "                     .count()\n",
    "                     .cache())\n",
    "df_protocol = (df_protocol\n",
    "                        .toPandas()\n",
    "                        .sort_values(by=['count'], ascending=False))\n",
    "df_protocol[\"Percentage\"] = 100.00 * (df_protocol[\"count\"] / sum(df_protocol[\"count\"]))\n",
    "df_protocol[\"Log(count)\"] = np.log(df_protocol[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_protocol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('pyspark_venv': venv)",
   "language": "python",
   "name": "python37564bitpysparkvenvvenv12caecce4d7944089893a0cc0cfe69ee"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
